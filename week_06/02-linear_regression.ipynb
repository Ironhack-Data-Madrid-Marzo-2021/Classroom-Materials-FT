{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confused-torture",
   "metadata": {},
   "source": [
    "# Regresi√≥n lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-wilson",
   "metadata": {},
   "source": [
    "![memesitodeld√≠a](../images/linear.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-chick",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Simple-linear-regression\" data-toc-modified-id=\"Simple-linear-regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Simple linear regression</a></span></li><li><span><a href=\"#Generamos-datos-para-explicar-de-forma-&quot;inversa&quot;-a-lo-visto-ayer-en-clase\" data-toc-modified-id=\"Generamos-datos-para-explicar-de-forma-&quot;inversa&quot;-a-lo-visto-ayer-en-clase-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Generamos datos para explicar de forma \"inversa\" a lo visto ayer en clase</a></span></li><li><span><a href=\"#Configuraciones-para-poner-mono-el-plot-de-seaborn\" data-toc-modified-id=\"Configuraciones-para-poner-mono-el-plot-de-seaborn-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Configuraciones para poner mono el plot de seaborn</a></span></li><li><span><a href=\"#¬øC√≥mo-de-bueno-es-nuestro-modelo?\" data-toc-modified-id=\"¬øC√≥mo-de-bueno-es-nuestro-modelo?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>¬øC√≥mo de bueno es nuestro modelo?</a></span></li><li><span><a href=\"#Calculamos-el-R2-del-modelo\" data-toc-modified-id=\"Calculamos-el-R2-del-modelo-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Calculamos el R2 del modelo</a></span></li><li><span><a href=\"#Regresi√≥n-lineal-con-sklearn\" data-toc-modified-id=\"Regresi√≥n-lineal-con-sklearn-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Regresi√≥n lineal con sklearn</a></span></li><li><span><a href=\"#Regresi√≥n-lineal-con-statsmodels\" data-toc-modified-id=\"Regresi√≥n-lineal-con-statsmodels-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Regresi√≥n lineal con statsmodels</a></span></li><li><span><a href=\"#Conceptos-del-OLS\" data-toc-modified-id=\"Conceptos-del-OLS-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Conceptos del OLS</a></span></li><li><span><a href=\"#Regresi√≥n-lineal-m√∫ltiple\" data-toc-modified-id=\"Regresi√≥n-lineal-m√∫ltiple-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Regresi√≥n lineal m√∫ltiple</a></span></li><li><span><a href=\"#Variables-categ√≥ricas\" data-toc-modified-id=\"Variables-categ√≥ricas-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Variables categ√≥ricas</a></span></li><li><span><a href=\"#Extensiones-del-modelo-lineal\" data-toc-modified-id=\"Extensiones-del-modelo-lineal-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Extensiones del modelo lineal</a></span><ul class=\"toc-item\"><li><span><a href=\"#Desafiando-la-suposici√≥n-aditiva:-la-sinergia\" data-toc-modified-id=\"Desafiando-la-suposici√≥n-aditiva:-la-sinergia-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Desafiando la suposici√≥n aditiva: la sinergia</a></span></li></ul></li><li><span><a href=\"#Selecci√≥n-de-modelo\" data-toc-modified-id=\"Selecci√≥n-de-modelo-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Selecci√≥n de modelo</a></span><ul class=\"toc-item\"><li><span><a href=\"#$R^2$-Ajustado\" data-toc-modified-id=\"$R^2$-Ajustado-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>$R^2$ Ajustado</a></span></li></ul></li><li><span><a href=\"#Selecci√≥n-por-pasos\" data-toc-modified-id=\"Selecci√≥n-por-pasos-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Selecci√≥n por pasos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Best-subset-selection\" data-toc-modified-id=\"Best-subset-selection-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>Best subset selection</a></span></li><li><span><a href=\"#Modelo-nulo\" data-toc-modified-id=\"Modelo-nulo-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>Modelo nulo</a></span></li><li><span><a href=\"#Forward-stepwise-selection\" data-toc-modified-id=\"Forward-stepwise-selection-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;</span>Forward stepwise selection</a></span></li></ul></li><li><span><a href=\"#Problemas-potenciales-en-la-regresi√≥n-lineal\" data-toc-modified-id=\"Problemas-potenciales-en-la-regresi√≥n-lineal-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Problemas potenciales en la regresi√≥n lineal</a></span></li><li><span><a href=\"#Resumen\" data-toc-modified-id=\"Resumen-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Resumen</a></span></li><li><span><a href=\"#Further-Materials\" data-toc-modified-id=\"Further-Materials-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Further Materials</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizaci√≥n\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib inline\n",
    "%config Inlinebackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "# Configuraciones de seaborn\n",
    "\n",
    "\n",
    "# Librer√≠as de modelado\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-therapy",
   "metadata": {},
   "source": [
    "## Simple linear regression\n",
    "\n",
    "Regresi√≥n lineal simple un modelo estad√≠stico que supone una relaci√≥n lineal entre un predictor y una variable objetivo. Matem√°ticamente, se puede expresar como:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-monthly",
   "metadata": {},
   "source": [
    "![formula](../images/formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-living",
   "metadata": {},
   "source": [
    "Si profundizamos un poco m√°s, podemos encontrar esta otra expresi√≥n:\n",
    "\n",
    " $$ Y = \\beta_0 +  \\beta_1 X + \\epsilon$$\n",
    "\n",
    "Donde:\n",
    " * $X$ = variable predictora\n",
    " * $Y$ = variable objetivo\n",
    " * $\\beta_0$ = intercept\n",
    " * $\\beta_1$ = pendiente / slope\n",
    " * $\\epsilon$ = ruido (gaussiano)\n",
    "\n",
    "\n",
    "La ecuaci√≥n anterior se conoce como *l√≠nea de regresi√≥n poblacional*.\n",
    "La l√≠nea de regresi√≥n lineal simple suele tener la forma que se muestra en la f√≥rmula anterior, donde Œ≤0 y Œ≤1 son constantes desconocidas, que representan el intercepto y la pendiente de la l√≠nea de regresi√≥n, respectivamente.\n",
    "\n",
    "El intercepto es el valor de la variable dependiente (Y) cuando la variable independiente (X) tiene un valor de cero (0). La pendiente es una medida de la velocidad a la que cambia la variable dependiente (Y) cuando la variable independiente (X) cambia en uno (1). Las constantes desconocidas se denominan coeficientes o par√°metros del modelo. Esta forma de la l√≠nea de regresi√≥n se conoce a veces como l√≠nea de regresi√≥n poblacional y, como modelo probabil√≠stico, se ajusta al conjunto de datos de forma aproximada de ah√≠ el uso del s√≠mbolo (‚âà) en la imagen. El modelo se denomina probabil√≠stico porque no modela toda la variabilidad de la variable dependiente (Y) : El modelo se llama probabil√≠stico porque no modela toda la variabilidad de la variable dependiente (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/hours_vs_mark.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hac√≠amos predicciones en base a una inferencia de beta_0 y beta_1 (m,n / pendiente, intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-pontiac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-monkey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calcul√°bamos el error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-bangkok",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-killing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "impossible-height",
   "metadata": {},
   "source": [
    "## Generamos datos para explicar de forma \"inversa\" a lo visto ayer en clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Del ejemplo visto ayer en clase \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-impression",
   "metadata": {},
   "source": [
    "Documentaci√≥n de np.random normal --> https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html    \n",
    "Par√°metros : \n",
    "- Media\n",
    "- Desviaci√≥n est√°ndar\n",
    "- Tama√±o de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos las horas de estudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos las notas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Generamos el dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-lodging",
   "metadata": {},
   "source": [
    "##¬†Configuraciones para poner mono el plot de seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "sns.set(rc={'figure.figsize': (16., 9.)})\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pintamos la l√≠nea de regresi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-lodge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-mandate",
   "metadata": {},
   "source": [
    "Por supuesto, en la vida real no conocemos los verdaderos par√°metros del modelo, ¬°¬°¬°ni si el modelo es real!!! Hoy vamos a aprender una [valiosa lecci√≥n](https://en.wikipedia.org/wiki/All_models_are_wrong):\n",
    "\n",
    "\n",
    "\n",
    "<center> <b>\"Todos los modelos son err√≥neos, pero algunos son √∫tiles\"</b> </center>\n",
    "\n",
    "\n",
    "En la pr√°ctica lo que hacemos es, tras ver un gr√°fico de dispersi√≥n como el de arriba, intentar inferir los par√°metros del modelo $\\beta_0$ y la pendiente, $\\beta_1$.  Una vez estimados, el ajuste estimado se convierte en $$ \\hat{Y} = \\hat{beta_0} + \\hat{beta_1} X$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-arrest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-paragraph",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-attitude",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-bottom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-anger",
   "metadata": {},
   "source": [
    "##¬†¬øC√≥mo de bueno es nuestro modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-church",
   "metadata": {},
   "source": [
    "La diferencia num√©rica entre la *l√≠nea de regresi√≥n de m√≠nimos cuadrados* y el valor real se llama *residuo* , y representa el error en la estimaci√≥n: $e = y_i - \\hat{y}$.     \n",
    "La l√≠nea de regresi√≥n minimiz√≥ la *Suma de cuadrados residual* (RSS)     \n",
    "\n",
    "$$RSS = e_1^2 + e_2^2 + \\dots + e_n ^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-luther",
   "metadata": {},
   "source": [
    "Si s√≥lo utilizamos la media como valor predicho para cada predicci√≥n, el error que cometer√≠amos es (*suma total de cuadrados*)\n",
    "\n",
    "$$TTS=\\Sigma(y_i - \\bar{y}_i)^2$$\n",
    "Consideremos esto nuestro punto de partida, hagamos una predicci√≥n y la ploteamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-container",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Vamos a meterlo en un dataframe para verlo m√°s claro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-objective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-massage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos TSS para el modelo anterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-roulette",
   "metadata": {},
   "source": [
    "Recordemos que los coeficientes de la regresi√≥n lineal minimizan el $RSS=Sigma(y_i - \\hat{y_i})^2$, es decir, la cantidad de variabilidad que queda sin explicar despu√©s de realizar la regresi√≥n. El [coeficiente de determinaci√≥n](https://en.wikipedia.org/wiki/Coefficient_of_determination):\n",
    "\n",
    "$$R^2 = \\frac{TSS -RSS}{TSS} = 1-\\frac{RSS}{TSS}$$\n",
    "\n",
    "mide la \"*proporci√≥n de variabilidad en Y que puede explicarse mediante X*\". Es una medida de la relaci√≥n lineal que existe entre $X$ e $y$.\n",
    "\n",
    "**Nota:** en el caso de la regresi√≥n lineal simple, el coeficiente $R^2$ no es m√°s que el cuadrado del coeficiente de correlaci√≥n de *Pearson* que ya conocemos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-namibia",
   "metadata": {},
   "source": [
    "##¬†Calculamos el R2 del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-lancaster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-constant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-miller",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-building",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-lighting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-magic",
   "metadata": {},
   "source": [
    "$R^2$ mide lo bueno que es nuestro modelo de regresi√≥n. Cuanto m√°s grande, mejor. Es un valor entre 0 y 1    \n",
    "**NOTA**: es computable para cualquier modelo, no importa si es lineal o no. S√≥lo se necesitan los valores reales y los predichos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-script",
   "metadata": {},
   "source": [
    "##¬†Regresi√≥n lineal con sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-egyptian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-stuff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-romantic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-flight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-prison",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "conscious-kingdom",
   "metadata": {},
   "source": [
    "** C√°lculo del error medio absoluto, del error medio cuadr√°tico y del error medio cuadr√°tico\n",
    "\n",
    "- **MAE** es el m√°s f√°cil de entender, porque es el error medio.\n",
    "- **El MSE** es m√°s popular que el MAE, porque el MSE \"castiga\" los errores m√°s grandes, lo que suele ser √∫til en el mundo real.\n",
    "- **RMSE** es a√∫n m√°s popular que MSE, es la ra√≠z cuadrada del MSE y mide la desviaci√≥n est√°ndar de los residuos.\n",
    "\n",
    "Todas estas son **funciones de p√©rdida**, porque queremos minimizarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-spanking",
   "metadata": {},
   "source": [
    "Lee m√°s sobre MAE, MSE, RMSE Y R2 [AQU√ç](http://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-albuquerque",
   "metadata": {},
   "source": [
    "## Regresi√≥n lineal con statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-values",
   "metadata": {},
   "source": [
    "Para no variar, un poquito de [documentaci√≥n](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html)      \n",
    "Y [este art√≠culo](https://jyotiyadav99111.medium.com/statistics-how-should-i-interpret-results-of-ols-3bde1ebeec01) que resume c√≥mo interpretar la informaci√≥n del summary del OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-tongue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "novel-portugal",
   "metadata": {},
   "source": [
    "##¬†Conceptos del OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-amazon",
   "metadata": {},
   "source": [
    "-  <b>R2</b> : El coeficiente de determinaci√≥n mide cuanta de la variaci√≥n de ùë¶ es explicada por el modelo.\n",
    "Si la varianza de los errores o residuales ùúé2ùëí es cero, el modelo explica el 100% de la variable ùë¶. Si ùúé2ùëí es igual a la varianza de ùë¶ el modelo no explica nada y ùëÖ2 vale cero.\n",
    "\n",
    "\n",
    "- <b>ùëÖ¬Ø2 </b> : El coeficiente de correlaci√≥n ajustado ùëÖ¬Ø2 corrige el valor de ùëÖ2 por la cantidad de variables ùëò (igual a 2 para el caso analizado) y la cantidad de datos  ùëÅ\n",
    "\n",
    "- <b>P value </b> El p-valor para cada t√©rmino comprueba la hip√≥tesis nula de que el coeficiente es igual a cero (no tiene efecto). Un p-valor bajo (< 0.05) indica que puedes rechazar la hip√≥tesis nula. ... T√≠picamente se utilizan los p-valores para determinar que t√©minos deben de mantenerse en el modelo de regresi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-russell",
   "metadata": {},
   "source": [
    "## Regresi√≥n lineal m√∫ltiple\n",
    "\n",
    "Por supuesto, las horas que uno estudia no son el √∫nico factor importante para sacar buenas notas en el mundo real. Podemos pensar en el cociente intelectual, por ejemplo, como otro factor determinante. De hecho, podemos generalizar un modelo lineal para tener tantas variables como queramos:\n",
    "\n",
    " $$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_2 X_m + \\epsilon$$\n",
    " \n",
    " En este caso lo que vamos a hacer, es a√±adir una variable que nos resta de la nota, las horas de fiesta.\n",
    " Imaginemos que por cada hora que salimos de fiesta mueren neuronas en nuestro cerebro y se nos olvida informaci√≥n, por tanto, nos restar√° nota (recordad que estamos inventando datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a crear el dataframe con las notas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-making",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-tokyo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-tradition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "popular-rachel",
   "metadata": {},
   "source": [
    "Los coeficientes de la regresi√≥n lineal m√∫ltiple se calculan de forma similar al caso de la regresi√≥n lineal simple: minimizan\n",
    "\n",
    "$$RSS = \\Sigma(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "\n",
    "donde:\n",
    "\n",
    " $$ \\hat{y} = \\hat{beta_0} + \\hat{beta_1 X_1} + \\hat{beta_2} X_2 + \\hat + \\hat{\\beta_2} X_m$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-grove",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos los resultados con el OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-allah",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "secure-natural",
   "metadata": {},
   "source": [
    "## Variables categ√≥ricas\n",
    "\n",
    "Muy a menudo nos enfrentamos a situaciones en las que los predictores son de naturaleza *cualitativa*. Un buen ejemplo podr√≠a ser el sexo de una persona, que puede tomar los valores $M$ o $F$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-presentation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "##¬†¬øC√≥mo trabajamos estas variables? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.get_dummies(data)\n",
    "#data.sex.str.get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-information",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "connected-phase",
   "metadata": {},
   "source": [
    "Incluimos esta informaci√≥n en el modelo a trav√©s de una variable *dummy*:\n",
    "$$\n",
    "x_i= \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1  \\quad \\text{si la persona es mujer} \\\\\n",
    "      0  \\quad \\text{si la persona es hombre} \\\\\n",
    "\\end{array} \n",
    "\\right. \n",
    "$$\n",
    "\n",
    "\n",
    "Si esta es nuestra √∫nica variable, esto resulta en un modelo:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\beta_0 + \\beta_1 +\\epsilon_i  \\quad \\text{si la persona es mujer} \\\\\n",
    "      \\beta_0 + \\epsilon_i  \\quad \\text{si la persona es hombre} \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "En este caso, $\\beta_0$ representa la nota media de los hombres, y $\\beta_0 + \\beta_1$ la nota media de las mujeres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-scheduling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acquired-steps",
   "metadata": {},
   "source": [
    "## Extensiones del modelo lineal\n",
    "\n",
    "Hay varios supuestos que se utilizan al ajustar un modelo lineal. \n",
    "* Los errores se distribuyen normalmente y tienen una varianza constante\n",
    "* Los errores no est√°n correlacionados entre s√≠\n",
    "* **Supuesto aditivo** El efecto de los cambios en un predictor $X_j$ sobre la respuesta $Y$ es independiente de los valores de los otros predictores.\n",
    "* **Supuesto lineal** El cambio en la respuesta para un aumento de una unidad en $X_j$ es el mismo sin importar el valor de $X_j$.\n",
    "\n",
    "### Desafiando la suposici√≥n aditiva: la sinergia\n",
    "\n",
    "A veces nuestras variables tendr√°n interacciones naturales. Por ejemplo, podemos pensar que cuanto m√°s se escuchen nuestros anuncios en la radio, m√°s eficaces ser√°n nuestros anuncios en la televisi√≥n. Es decir, el efecto de ambos es *mayor* (o *menor*) que la suma de las partes.\n",
    "\n",
    "Este es un tema com√∫nmente estudiado en marketing](https://smallbusiness.chron.com/definition-synergy-marketing-21786.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adv = pd.read_csv('../datasets/Advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-australia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-baghdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cordless-knock",
   "metadata": {},
   "source": [
    "La diferencia es que la covarianza nos da la direcci√≥n (positiva o negativa) entre las variables y la correlaci√≥n nos da esto m√°s la fuerza de esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-tourism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "disabled-quarter",
   "metadata": {},
   "source": [
    "Recuerde el **principio jer√°rquico:**\n",
    "\n",
    "\"*Si incluimos una interacci√≥n en un modelo, debemos incluir tambi√©n los efectos principales, incluso si los valores p que se asocian a sus coeficientes no son significativos*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-cleaning",
   "metadata": {},
   "source": [
    "## Selecci√≥n de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-saturday",
   "metadata": {},
   "source": [
    "###  $R^2$ Ajustado \n",
    "Hay una cosa curiosa con $R^2$. ¬°¬°Mira lo que pasa cuando incluimos variables *al azar*!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adv['rand_1'] = np.random.normal(size=200)\n",
    "df_adv['rand_2'] = np.random.normal(size=200)\n",
    "df_adv['rand_3'] = np.random.normal(size=200)\n",
    "df_adv['rand_4'] = np.random.normal(size=200)\n",
    "df_adv['rand_5'] = np.random.normal(size=200)\n",
    "df_adv['rand_6'] = np.random.normal(size=200)\n",
    "df_adv['rand_7'] = np.random.normal(size=200)\n",
    "df_adv['rand_8'] = np.random.normal(size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-cannon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-receiver",
   "metadata": {},
   "source": [
    "El coeficiente `Adj. R-cuadrado` pretende penalizar el $R^2$ de un modelo cuando se incluyen *demasiadas* varaibles. \n",
    "$$\\bar R^2 = 1-(1-R^2){n-1 \\ sobre n-p-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-participation",
   "metadata": {},
   "source": [
    "##¬†Selecci√≥n por pasos\n",
    "Siempre hay que intentar tener un modelo lo m√°s sencillo posible. Habr√° otras formas de hacerlo utilizando la **regularizaci√≥n**, veremos m√°s adelante c√≥mo hay librer√≠as que nos ayudar√°n a decidir si nos quedamos con unas u otras variabless, pero hasta ahora los m√©todos que describimos aqu√≠ son bastante √∫tiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-wallet",
   "metadata": {},
   "source": [
    "### Best subset selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-constant",
   "metadata": {},
   "source": [
    "El proceso de best subset selection consiste en evaluar todos los posibles modelos que se pueden crear por combinaci√≥n de los predictores disponibles. El algoritmo a seguir para k predictores es:\n",
    "\n",
    "- Se genera lo que se conoce como modelo nulo (M0), que es el modelo sin ning√∫n predictor.\n",
    "\n",
    "- Se generan todos los posibles modelos que contienen un √∫nico predictor y se selecciona el que tiene menor error de entrenamiento. Al modelo seleccionado se denomina (M1).\n",
    "\n",
    "- Se repite el paso anterior para modelos con dos predictores y as√≠ sucesivamente hasta llegar al modelo con todos los predictores (Mk).\n",
    "\n",
    "- De entre los mejores modelos seleccionados para cada n√∫mero de predictores (M0, M1, M2,‚Ä¶,Mk) se identifica el mejor modelo, esta vez empleando una m√©trica de validaci√≥n (R2 Ajustado).     \n",
    "\n",
    "A pesar de que este m√©todo explora todas las posibilidades, tiene dos limitaciones fundamentales:\n",
    "Requerimientos computacionales: Se requiere calcular 2p modelos distintos, lo que lo hace inviable para m√°s de 40 predictores.\n",
    "Problemas de overfitting. Al generarse tantos modelos, por simple azar se pueden encontrar buenos resultados. Por esta raz√≥n best subset selection no se ecominda si hay m√°s de 10 predictores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-practice",
   "metadata": {},
   "source": [
    "###¬†Modelo nulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-museum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-violence",
   "metadata": {},
   "source": [
    "### Forward stepwise selection\n",
    "\n",
    "Forward stepwise selection es una alternativa computacionalmente m√°s eficiente que best subset selection, en la que no se eval√∫an todas las posibles combinaciones de predictores sino solo un subconjunto. El proceso se inicia generando el modelo nulo (M0) sin predictores. A continuaci√≥n, se generan todos los posibles modelos que se pueden crear a√±adiendo un predictor al modelo nulo. De entre todos estos modelos con 1 predictor se selecciona el mejor bas√°ndose en el error de entrenamiento, al modelo elegido se denomina M1. Se repite el paso anterior, pero esta vez partiendo del √∫ltimo modelo seleccionado y as√≠ sucesivamente hasta llegar al modelo con todos los predictores. De entre los mejores modelos seleccionados para cada n√∫mero de predictores (M0, M1, M2,‚Ä¶,Mk), se identifica el mejor, esta vez empleando una m√©trica de validaci√≥n (validaci√≥n cruzada, Cp, AIC, BIC o R2ajustado).\n",
    "\n",
    "Al crear modelos anidados, en los que el modelo k se construye a partir del modelo k‚àí1, el m√©todo forward stepwise selection no garantiza que se seleccione el mejor modelo de entre todos los posibles, ya que no se eval√∫an todas las posibles combinaciones. Sin embargo, suele llegar a modelos √≥ptimos consiguiendo un buen rendimiento computacional y evitando el overfitting. Otra ventaja a√±adida es que, forward stepwise selection puede emplearse incluso cuando el n√∫mero de predictores es mayor que el de observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-institution",
   "metadata": {},
   "source": [
    "Primero:  \n",
    "1. Todos los modelos con una sola variable. Uno gana. Ganador A\n",
    "2. A√±ade a este modelo todas las variables, una por una. Uno gana. Ganador B \n",
    "3. A√±ade a este modelo todas las variables, una por una. Uno gana. Ganador C\n",
    "...\n",
    "...\n",
    "\n",
    "FIN: tomar el ganador entre A, B, C, D...\n",
    "\n",
    "Si R2 tiene dos decimales iguales, consideramos el modelo con m√°s variables representativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-province",
   "metadata": {},
   "source": [
    "## Problemas potenciales en la regresi√≥n lineal\n",
    "\n",
    "Los principales supuestos de un modelo lineal son:\n",
    "\n",
    "* Los datos son lineales \n",
    "* Los errores no est√°n correlacionados\n",
    "* La varianza de los t√©rminos de error es constante\n",
    "\n",
    "¬øQu√© ocurre si no se cumplen estos supuestos? \n",
    "\n",
    "Adem√°s, nuestros modelos pueden sufrir otros problemas como:\n",
    "* Valores at√≠picos\n",
    "* Puntos de apalancamiento elevados\n",
    "* Colinealidad\n",
    "* Valores perdidos\n",
    "\n",
    "Ved este [v√≠deo](https://www.youtube.com/watch?v=hVe2F9krrWk) para una introducci√≥n al tema.     \n",
    "Leed la secci√≥n 3.3.3 del libro [Introducci√≥n al aprendizaje estad√≠stico](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-invasion",
   "metadata": {},
   "source": [
    "![anscombe](../images/anscombe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-nancy",
   "metadata": {},
   "source": [
    "Echa un ojo a [Wikipedia](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) que dice que:       \n",
    "El cuarteto de Anscombe consta de cuatro conjuntos de datos que tienen estad√≠sticas descriptivas simples casi id√©nticas, pero que tienen distribuciones muy diferentes y aparecen de forma muy distinta cuando se grafican. Cada conjunto de datos consta de once puntos (x,y). Fueron construidos en 1973 por el estad√≠stico Francis Anscombe para demostrar tanto la importancia de graficar los datos antes de analizarlos como el efecto de los valores at√≠picos y otras observaciones influyentes en las propiedades estad√≠sticas. Describi√≥ el art√≠culo como un intento de contrarrestar la impresi√≥n entre los estad√≠sticos de que \"los c√°lculos num√©ricos son exactos, pero los gr√°ficos son toscos\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-uniform",
   "metadata": {},
   "source": [
    "##¬†Resumen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-enough",
   "metadata": {},
   "source": [
    "## Further Materials \n",
    "\n",
    "* [Introduction to Statistical Learning (Chapter 3)](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)\n",
    "* One example of [linear regression with the Boston data set](https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-stand",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
